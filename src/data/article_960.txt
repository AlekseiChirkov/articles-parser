Название: Копируем файлы пачками в AWS S3

Автор: romblin

Дата: 2021-09-15, 08:00

Теги: Python *, Программирование *, Amazon Web Services *, Промышленное
программирование *, Разработка под Linux *

Контент: В одном из проектов встала следующая задача: пользователь загружает
пачку файлов через клиента (CloudBerry Explorer, к примеру) в S3
бакет, мы копируем эти файлы в архив и шлем SNS уведомление о том, что
все сделано. Перекладывать файлы в архив нужно начинать только тогда,
когда пользователь загрузит все, что хотел. Пользователей мало и
загружают батчи они довольно редко. Но файлов может быть много.Чтобы
понять, что пора начинать архивацию, зададим определенную структуру
каталогов и будем просить пользователя загружать триггер-файлы с
расширением .trigger когда он закончит. Этакая эмуляция кнопки Done.
Структура каталогов будет такой:<batch_name>/done.trigger
<batch_name>/files/<file_key_1> <batch_name>/files/<file_key_2> ...
<batch_name>/files/<file_key_n>Как видим, для каждой пачки создается
свой каталог <batch_name> с подкаталогом files, в который и заливаются
уже пользовательские файлы с каталогами и именами, которые он хочет.
Триггер-файл загружается в <batch_name> и по ключу этого  файла можно
понять какие конкретно файлы нужно отправить в архив. Но здесь есть
один нюанс, мы хотим при копировании в архив вырезать каталог files.
Т.е. файл <batch_name>/files/<file_key_1> скопировать в
<batch_name>/<file_key_1>.К счастью, S3 позволяет отслеживать загрузку
файлов с определенным суффиксом и отправлять уведомления при
наструплении этого события. В качестве получаетеля этих уведомлений
можно указать аж 3 сервиса: SNS, SQS и Lambda-функцию. Но тут не без
нюансов. Так, первые 2 типа поддерживают только стандартные очереди и
SNS, а FIFO не поддерживают, увы.