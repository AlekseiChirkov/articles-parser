Название: Involution: Attention is not what you need, или Как скрестить Self-
Attention из NLP и Convolution в задачах CV

Автор: nagadit

Дата: 2021-09-16, 13:15

Теги: Python *, Алгоритмы *, Машинное обучение *

Контент: Если говорить про Self-Attention в картиночных моделях, то тут есть 2
варианта. Олдскульный  “давайте просто перевзвесим фичи” в разных
вариантах: поканально, пространственно, в некоторой проекции. И
новомодный "давайте обучим трансформер" с представлением патчей как
визуальных слов. Первый подход рабочий, но не дает значительного
улучшения в плане метрик. Второй подход слишком вычислительно сложный
и часто заточен на размер картинок.  Подход коллег из ByteDance AI Lab
и университета Пекина сильно отличается от этих крайностей и является
переосмыслением Attention-механизма трансформеров в работе свёрток.