Название: REALM — интеграция извлеченной информации в модели языковых
представлений

Автор: Kouki_RUS

Дата: 2021-09-15, 09:37

Теги: Машинное обучение *

Контент: Последние достижения в области обработки естественного языка (Natural
Language Processing, NLP) в значительной степени основаны на успехах
предварительного обучения без учителя, с помощью которого можно
обучать универсальные языковые модели на большом количестве текстов
без ручной разметки или меток. Было показано, что такие предобученные
модели, вроде BERT и RoBERTa, запоминают удивительно большое
количество общих знаний о мире, например «место рождения Франческо
Бартоломео Конти», «разработчик JDK» и «владелец Border TV». Хотя
способность кодировать знания особенно важна для определенных задач
обработки естественного языка, таких как ответы на вопросы, поиск
информации и генерация текста, эти модели запоминают знания неявно, т.
е. знания о мире фиксируются абстрактным образом в весах модели, что
затрудняет определение, какие знания были сохранены и где именно они
хранятся в модели. Кроме того, объем памяти и, следовательно, точность
модели ограничены размером нейронной сети. Чтобы получить больше
знаний о мире, стандартной практикой является обучение все более
крупных сетей, что, однако, может сильно замедлять и удорожать
процесс.